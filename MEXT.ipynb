{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT generated stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import openai\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.key = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw stories collected from web => function => formatted json (id, title, story, vocab) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read raw stories into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:/Yash/Documents/MEXT/raw_data.txt\", \"r\") as file:\n",
    "    content = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(content):\n",
    "    data = {\"ids\":{}}\n",
    "    for i in range(len(content)):\n",
    "        if re.search('^[0-9].', content[i]):                            # if number. present in string -> title\n",
    "            data['ids'][i] = {}\n",
    "            title = content[i]\n",
    "            title = title.replace('\\n', '')\n",
    "            p = re.search('^[0-9]*.', title)                            # store index of story number\n",
    "            title = title[p.end()+1:]                                   # extract only words from title *                  \n",
    "\n",
    "            raw = content[i+1]\n",
    "            story = re.sub('\\.', '. ', raw).replace('  ', ' ')          # 'finally.He' -> 'finally. He' \n",
    "            words = re.sub(r'[^\\w\\s]', '', story)                       # remove all punctuations from passage\n",
    "            story_vocab = list(set(words.lower().split()))              # lower all words and take unique words\n",
    "            \n",
    "            data['ids'][i][\"title\"] = title\n",
    "            data['ids'][i][\"story\"] = story\n",
    "            data['ids'][i][\"vocab\"] = story_vocab\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = format_data(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg words in a story\n",
    "x = 0\n",
    "for i in data['ids'].keys():\n",
    "    story = re.sub('\\.', ' ', data['ids'][i]['story']).replace('  ', ' ')\n",
    "    story = re.sub(r'[^\\w\\s]', '', story)\n",
    "    x += len(story.split(' '))\n",
    "    \n",
    "avg_tokens = x/20 * 1.35                                 # since 1 word ~ 1.35 tokens as per GPT 3 docs\n",
    "avg_tokens   # in a story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store json formatted story data with vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/Yash/Documents/MEXT/story_data.txt', 'w') as file:\n",
    "    file.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receive gpt generated stories on the given vocab\n",
    "### Input vocab => gpt => gpt generated stories => analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read json formatted story data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/Yash/Documents/MEXT/story_data.txt') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {\"Generate a short story using the following words:\":'a',  # being too lineent and allowing gpt to be flexible\n",
    "           \"Generate a story within 250 words using the following words:\":'b', \n",
    "           \"Generate a story using the given words and other helping words if you may require:\":'c', \n",
    "           \"Generate a story by strictly trying to use the words only given in this list:\":'d',\n",
    "           \"Generate a story using only the words given in this list:\":'e',\n",
    "           \"Generate a complete story within 250 words strictly using only the words present in this list:\":'f'} # too rigid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT response class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpt_stories():\n",
    "    \n",
    "    sws = ['!','&','-','.',',','/','?','“', '”', '‘', '’', \"'\", '\"', ':', ';', ']', '[', '_', '(', ')', '$', '']\n",
    "    def __init__(self, key):\n",
    "        openai.api_key = key\n",
    "        \n",
    "    def get_stories(self, data):\n",
    "        self.gpt_stories = {\"model_params\":{\"model\":\"text-davinci-003\", \"temperature\":0.7, \"max_tokens\":300, \"top_p\":1},\n",
    "                            \"ids\":{}}\n",
    "\n",
    "        for pr in prompts.keys():\n",
    "            for i in data['ids'].keys():\n",
    "                prompt = f\"{pr}\\n{data['ids'][i]['vocab']}\"\n",
    "                response = openai.Completion.create(model=\"text-davinci-003\",\n",
    "                                                    prompt=prompt,\n",
    "                                                    temperature=0.7,\n",
    "                                                    max_tokens=300,\n",
    "                                                    top_p=1,\n",
    "                                                    frequency_penalty=0,\n",
    "                                                    presence_penalty=0)\n",
    "                \n",
    "                story = response['choices'][0]['text']\n",
    "                \n",
    "                self.gpt_stories['ids'][f\"{prompts[pr]}-{i}\"] = {}\n",
    "                self.gpt_stories['ids'][f\"{prompts[pr]}-{i}\"][\"prompt\"] = prompt\n",
    "                self.gpt_stories['ids'][f\"{prompts[pr]}-{i}\"][\"story\"] = story\n",
    "                \n",
    "                story = re.sub(\"['\\-', '/']\", ' ', story)                      # replace - & / with whitespace\n",
    "#                 story = re.sub(r'[^\\w\\s]', '', story)                          # remove all punctuations from passage\n",
    "#                 story_words = list(set(story.lower().split()))\n",
    "                words = [word for word in word_tokenize(story.lower()) if word not in stopwords.words('english')]\n",
    "                vocab = set(words)\n",
    "                for j in self.sws:\n",
    "                    if j in vocab:\n",
    "                        vocab.remove(j)\n",
    "                \n",
    "                self.gpt_stories['ids'][f\"{prompts[pr]}-{i}\"][\"vocab\"] = vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = gpt_stories(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.get_stories(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save gpt generated story data as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/Yash/Documents/MEXT/gpt_response_2.txt', 'w') as file:\n",
    "    file.write(json.dumps(gpt.gpt_stories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze GPT generated responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read gpt generated response into memory\n",
    "#### gpt_response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/Yash/Documents/MEXT/gpt_response_1.txt', 'r') as file:\n",
    "    gpt_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_vocab = []\n",
    "for key in data['ids'].keys():\n",
    "    v = data['ids'][key]['vocab']\n",
    "    story_vocab.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(story_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gpt_data['ids'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the keys of gpt_response_1 before caluculating accuracy\n",
    "accuracies = []\n",
    "for i,j in enumerate(gpt_data['ids'].keys()):\n",
    "    acc = 0\n",
    "    for word in gpt_data['ids'][j]['vocab']:\n",
    "        if re.search(word, data['ids'][j.split('-')[1]]['story']):\n",
    "            acc += 1\n",
    "    \n",
    "    index = int(int(j.split('-')[1])/3)\n",
    "    accuracies.append(round(acc/len(data['ids'][j.split('-')[1]]['vocab']) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data={'ID':list(gpt_data['ids'].keys()),\n",
    "                        'ip_story_wc': [len(data['ids'][i]['story'].split()) for i in data['ids'].keys()]*len(prompts),\n",
    "                        'ip_vocab_count':[len(story_vocab[i]) for i in range(len(story_vocab))]*len(prompts),\n",
    "                        'op_story_wc': [len(gpt_data['ids'][i]['story'].split()) for i in gpt_data['ids'].keys()],\n",
    "                        'op_vocab_count':[len(gpt_data['ids'][i]['vocab']) for i in gpt_data['ids'].keys()],\n",
    "                        'accuracy': accuracies})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg accuracies of each prompt using method 1\n",
    "for i in range(6):\n",
    "    print(list(prompts.keys())[i], end=' - ')\n",
    "    print(round(df2.iloc[i*20:i*20 + 20, 5].mean(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gpt_response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/Yash/Documents/MEXT/gpt_response_2.txt', 'r') as file:\n",
    "    gpt_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forgot to add list of vocab in the prompt, so doing it now. Do not run again\n",
    "story_vocab = []\n",
    "for key in data['ids'].keys():\n",
    "    v = data['ids'][key]['vocab']\n",
    "    story_vocab.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forgot to add list of vocab in the prompt, so doing it now. Do not run again\n",
    "len(story_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forgot to add list of vocab in the prompt, so doing it now. Do not run again\n",
    "len(gpt_data['ids'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forgot to add list of vocab in the prompt, so doing it now. Do not run again\n",
    "k = 0\n",
    "for i,j in enumerate(gpt_data['ids'].keys()):\n",
    "    if i%20==0:\n",
    "        k=0\n",
    "    gpt_data['ids'][j]['prompt'] += f\"\\n{story_vocab[k]}\"\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forgot to add list of vocab in the prompt, so doing it now. Do not run again\n",
    "with open('D:/Yash/Documents/MEXT/gpt_response_2.txt', 'w') as file:\n",
    "    file.write(json.dumps(gpt_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "# Testing accuracy through re.search and simple method using in keyword in python i.e if the words exists in any way in the \n",
    "# story text\n",
    "correct = []\n",
    "for word in gpt_data['ids']['a-0']['vocab']:\n",
    "    if re.search(word, data['ids']['0']['story']):  # Finding if the word exists in any form in the story text.\n",
    "        correct.append(word) # Pro - finds believe in believes -> therefore word should count towards accuracy\n",
    "                             # Con - finds all in really -> which shouldnt be counted towards accuracy and in in entertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(correct) / len(story_vocab[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "# Testing accuracy if the word exactly exists in the story vocab list\n",
    "correct2 = []\n",
    "for word in gpt_data['ids']['a-0']['vocab']:\n",
    "    if word in data['ids']['0']['vocab']: # Finding if the words occurs exactly in the vocab list of story (which in itself may have some flaws)\n",
    "        correct2.append(word)  # Con - misses out on too many words such as believe even tho there is believes in text\n",
    "                               # didn't (this exists coz of the way i format text into word list) doesn't get counted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(correct2) / len(story_vocab[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the words (in any form) that get counted towards accuracy when searching through method1 but neglected when searching \n",
    "# through method2\n",
    "set(correct).difference(set(correct2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "for i,j in enumerate(gpt_data['ids'].keys()):\n",
    "    acc = 0\n",
    "    for word in gpt_data['ids'][j]['vocab']:\n",
    "        if re.search(word, data['ids'][j.split('-')[1]]['story']):\n",
    "            acc += 1\n",
    "    \n",
    "    index = int(int(j.split('-')[1])/3)\n",
    "    accuracies.append(round(acc/len(data['ids'][j.split('-')[1]]['vocab']) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data={'ID':list(gpt_data['ids'].keys()),\n",
    "                        'ip_story_wc': [len(data['ids'][i]['story'].split()) for i in data['ids'].keys()]*len(prompts),\n",
    "                        'ip_vocab_count':[len(story_vocab[i]) for i in range(len(story_vocab))]*len(prompts),\n",
    "                        'op_story_wc': [len(gpt_data['ids'][i]['story'].split()) for i in gpt_data['ids'].keys()],\n",
    "                        'op_vocab_count':[len(gpt_data['ids'][i]['vocab']) for i in gpt_data['ids'].keys()],\n",
    "                        'accuracy': accuracies})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg accuracies of each prompt using method 1\n",
    "for i in range(6):\n",
    "    print(list(prompts.keys())[i], end=' - ')\n",
    "    print(round(df2.iloc[i*20:i*20 + 20, 5].mean(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 0\n",
    "for i in range(len(list(zip(*gpt.gpt_stories))[0])):\n",
    "    story = re.sub('\\.', ' ', list(zip(*gpt.gpt_stories))[0][i]).replace('  ', ' ')\n",
    "    story = re.sub(r'[^\\w\\s]', '', story)\n",
    "    y += len(story.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_tokens = y/20 * 1.35\n",
    "avg_tokens   # in a story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An avg accuracy of 66.77 where accuracy is defined as the percentage of words from a particular gpt generated story that\n",
    "# were present in the original story. With GPT params:\n",
    "# max_tokens = 300\n",
    "# temperature = 0.7\n",
    "# top_p = 1\n",
    "# prompt = Generate a short story of upto 250 words using the following words: <list of words>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An avg accuracy of 66.57 where accuracy is defined as the percentage of words from a particular gpt generated story that\n",
    "# were present in the original story. With GPT params:\n",
    "# max_tokens = 300\n",
    "# temperature = 0.7\n",
    "# top_p = 1\n",
    "# prompt = Generate a short story using the following words: <list of words>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Stop words\n",
    "### Input vocab => gpt => gpt generated stories => analyze\n",
    "### Receive gpt generated stories on the given vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read json formatted story data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/Yash/Documents/MEXT/data/story_data.txt') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words from story\n",
    "data_without_sw = {\"ids\":{}}\n",
    "sws = ['!','&','-','.',',','/','?','“', '”', '‘', '’', \"'\", '\"', ':', ';', ']', '[', '_', '(', ')', '$', '']\n",
    "for _id in data[\"ids\"]:\n",
    "    data_without_sw['ids'][_id] = {}\n",
    "    data_without_sw['ids'][_id]['title'] = data['ids'][_id]['title']\n",
    "    data_without_sw['ids'][_id]['story'] = data['ids'][_id]['story']\n",
    "    words = [word for word in word_tokenize(data['ids'][_id]['story'].lower()) if word not in stopwords.words('english')]\n",
    "    vocab = set(words)\n",
    "    for j in sws:\n",
    "        if j in vocab:\n",
    "            vocab.remove(j)\n",
    "    data_without_sw['ids'][_id]['vocab'] = list(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store json formatted story data with vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save story data without stop words to use in gpt input\n",
    "with open('D:/Yash/Documents/MEXT/data/story_data_wo_sw.txt', 'w') as file:\n",
    "    file.write(json.dumps(data_without_sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input vocab without stop words => GPT => stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read json formatted story data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/Yash/Documents/MEXT/data/story_data_wo_sw.txt') as file:\n",
    "    data2 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {\"Generate a short story using the following words:\":'a',  # being too linient and allowing gpt to be flexible\n",
    "           \"Generate a story within 250 words using the following words:\":'b', \n",
    "           \"Generate a story using the given words and other helping words if you may require:\":'c', \n",
    "           \"Generate a story by strictly trying to use the words only given in this list:\":'d',\n",
    "           \"Generate a story using only the words given in this list:\":'e',\n",
    "           \"Generate a complete story within 250 words strictly using only the words present in this list:\":'f'} # too rigid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = gpt_stories(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.get_stories(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save gpt generated story data as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting vocab to list otherwise wasn't able to store it as json\n",
    "for i in gpt.gpt_stories['ids']:\n",
    "    gpt.gpt_stories['ids'][i]['vocab'] = list(gpt.gpt_stories['ids'][i]['vocab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save gpt generated stories as json\n",
    "with open('D:/Yash/Documents/MEXT/data/gpt_response_3.txt', 'w') as file:\n",
    "    file.write(json.dumps(gpt.gpt_stories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze GPT generated responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read gpt generated response into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "sws = ['!','&','-','.',',','/','?','“', '”', '‘', '’', \"'\", '\"', ':', ';', ']', '[', '_', '(', ')', '$', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/Yash/Documents/MEXT/data/gpt_response_3.txt', 'r') as file:\n",
    "    gpt_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove sw from vocab of gpt_data, Do not run again.\n",
    "for _id in gpt_data['ids']:\n",
    "    for sw in sws:\n",
    "        if sw in gpt_data['ids'][_id]['vocab']:\n",
    "            gpt_data['ids'][_id]['vocab'].remove(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save new gpt_data\n",
    "with open('D:/Yash/Documents/MEXT/data/gpt_response_3.txt', 'w') as file:\n",
    "    file.write(json.dumps(gpt_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read gpt_response 3 into memory\n",
    "with open('D:/Yash/Documents/MEXT/data/gpt_response_3.txt', 'r') as file:\n",
    "    gpt_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read story data to compare with gpt response vocab\n",
    "with open('D:/Yash/Documents/MEXT/data/story_data_wo_sw.txt', 'r') as file:\n",
    "    story_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding difference b/w accuracies when word is searched in the story and when searched in the vocab list\n",
    "dummy_df = pd.DataFrame(columns=['story', 'vocab'])\n",
    "story_acc = []\n",
    "vocab_acc = []\n",
    "for j in gpt_data['ids'].keys():\n",
    "    acc = 0\n",
    "    acc2 = 0\n",
    "    for word in gpt_data['ids'][j]['vocab']:\n",
    "        if word in story_data['ids'][j.split('-')[1]]['vocab']:     # if the word is present in vocab list of prompt\n",
    "            acc+=1\n",
    "            \n",
    "        if word in story_data['ids'][j.split('-')[1]]['story'].lower():     # if the word is present in story of prompt\n",
    "            acc2+=1\n",
    "            \n",
    "    vocab_acc.append(round(acc/len(story_data['ids'][j.split('-')[1]]['vocab']) * 100, 2))\n",
    "    story_acc.append(round(acc2/len(story_data['ids'][j.split('-')[1]]['vocab']) * 100, 2))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df.story = story_acc\n",
    "dummy_df.vocab = vocab_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67.53</td>\n",
       "      <td>59.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74.58</td>\n",
       "      <td>57.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72.88</td>\n",
       "      <td>64.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76.27</td>\n",
       "      <td>74.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.79</td>\n",
       "      <td>63.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   story  vocab\n",
       "0  67.53  59.74\n",
       "1  74.58  57.63\n",
       "2  72.88  64.41\n",
       "3  76.27  74.58\n",
       "4  65.79  63.16"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>120.00000</td>\n",
       "      <td>120.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>75.11200</td>\n",
       "      <td>69.501167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.29748</td>\n",
       "      <td>11.552891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>47.17000</td>\n",
       "      <td>41.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>64.90250</td>\n",
       "      <td>60.907500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>73.57000</td>\n",
       "      <td>68.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>84.79000</td>\n",
       "      <td>77.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>102.04000</td>\n",
       "      <td>93.880000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           story       vocab\n",
       "count  120.00000  120.000000\n",
       "mean    75.11200   69.501167\n",
       "std     12.29748   11.552891\n",
       "min     47.17000   41.510000\n",
       "25%     64.90250   60.907500\n",
       "50%     73.57000   68.420000\n",
       "75%     84.79000   77.655000\n",
       "max    102.04000   93.880000"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anlyzing differnce\n",
    "story_num = {'num':0, 'difference':[]}\n",
    "vocab_num = {'num':0, 'difference':[]}\n",
    "for i,r in dummy_df.iterrows():\n",
    "    if r['story'] > r['vocab']:                                    # instance where story accuracy > vocab accuracy\n",
    "        story_num['num']+=1                                        # update count\n",
    "        story_num['difference'].append(r['story'] - r['vocab'])    # append the difference\n",
    "        \n",
    "    else:                                                          # instance where story accuracy < vocab accuracy\n",
    "        vocab_num['num']+=1                                        # update count\n",
    "        vocab_num['difference'].append(r['vocab'] - r['story'])    # append the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 avg_difference: 5.906140350877192\n",
      "6 avg_difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(story_num['num'], 'avg_difference:', sum(story_num['difference'])/len(story_num['difference']))\n",
    "print(vocab_num['num'], 'avg_difference:', sum(vocab_num['difference'])/len(vocab_num['difference']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 114 out of 120 instances were observed where accuracy was greater when word was \n",
    "# searched in the story as compared to vocab list by an avg of 5.9%.\n",
    "\n",
    "# Therefore we'll search the words in the story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if the words in vocab list in gpt_response 3 are 100% present in the story\n",
    "_id = 'e-15'\n",
    "for word in gpt_data['ids'][_id]['vocab']:\n",
    "    if not re.search(word, gpt_data['ids'][_id]['story'].lower()):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes they are. GOod thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "for i,j in enumerate(gpt_data['ids'].keys()):\n",
    "    acc = 0\n",
    "    for word in gpt_data['ids'][j]['vocab']:\n",
    "        if re.search(word, story_data['ids'][j.split('-')[1]]['story'].lower()):\n",
    "            acc += 1\n",
    "    \n",
    "    accuracies.append(round(acc/len(story_data['ids'][j.split('-')[1]]['vocab']) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(data={'ID':list(gpt_data['ids'].keys()),\n",
    "                        'ip_story_wc': [len(story_data['ids'][i]['story'].split()) for i in story_data['ids'].keys()]*len(prompts),\n",
    "                        'ip_vocab_count':[len(story_data['ids'][i]['vocab']) for i in story_data['ids'].keys()]*len(prompts),\n",
    "                        'op_story_wc': [len(gpt_data['ids'][i]['story'].split()) for i in gpt_data['ids'].keys()],\n",
    "                        'op_vocab_count':[len(gpt_data['ids'][i]['vocab']) for i in gpt_data['ids'].keys()],\n",
    "                        'accuracy': accuracies})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a short story using the following words: - 73.42\n",
      "Generate a story within 250 words using the following words: - 72.82\n",
      "Generate a story using the given words and other helping words if you may require: - 72.64\n",
      "Generate a story by strictly trying to use the words only given in this list: - 74.3\n",
      "Generate a story using only the words given in this list: - 79.41\n",
      "Generate a complete story within 250 words strictly using only the words present in this list: - 78.56\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(prompts):\n",
    "    print(j, end=' - ')\n",
    "    print(round(df3.iloc[i*20:i*20+20].accuracy.mean(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
